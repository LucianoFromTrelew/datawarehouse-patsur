%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% En 'inclues.tex' se encuentran la importación de paquetes necesarios
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{includes}
\input{listings_settings}


\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% En 'titlepage.tex' se encuentra la página de título
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{titlepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INDICE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\tableofcontents
\clearpage 

\lstset{style=sql}

\section{Introducción}

Para el siguiente trabajo de laboratorio se solicitaba implementar las bases de datos operacionales y un datawarehouse que satisfaga las necesidades de la empresa \dq{PatSur}. Los requerimientos son los siguientes:

\begin{displayquote}\itshape

La cooperativa frutihorticola “PatSur” lleva operando 5 años en la región patagónica. Ha ido creciendo y abriendo sucursales en otras ciudades y participa en la provisión de mercaderías a fabricas, otras cooperativas, pymes, cadenas de supermercados. A medida que fue ampliando su ámbito de compra y venta ha ido modifcando sus sistemas para manejar nuevos datos como producciones, vendedores y compradores, lugares de compra y venta, transportes, empleados, etc. De allí que cada Gerente de área posea distintos datos en diferentes formas. Por lo tanto la primer idea es tener información sobre las ventas. 

La empresa cuenta con dos (2) sistemas distintos de facturación de productos. A partir del año 2010 se implementó un sistema desarrollado por una consultora, mientras que antes del 2010 se utilizaba un sistema desarrollado por el sobrino de uno de los socios de la cooperativa.  

\end{displayquote}

Las implementaciones de todos los sistemas necesarios fueron hechas con contenedores de \docker{} y orquestrados con \doccom{}; se deja a disposición con el presente informe y todos los \emph{scripts} utilizados el archivo de configuración para levantar todos los contenedores necesarios. 

Los cuatro servicios creados por contenedores (un contenedor por servicio) se encuentran dentro de la red \textbf{172.40.0.0/16}, cada uno con las siguientes características:
\begin{itemize}
    \item Sistema de facturación viejo:
        \begin{itemize}
            \item Imagen: \underline{\href{https://hub.docker.com/_/postgres/}{PostgreSQL 9.6}}
            \item Dirección IP: 172.40.0.10
            \item Puerto de escucha: 5432
            \item Nombre de la base de datos: facturacion
            \item Usuario: admin
            \item Contraseña: admin
        \end{itemize}
    \item Sistema de facturación actual
        \begin{itemize}
            \item Imagen: \underline{\href{https://hub.docker.com/_/postgres/}{PostgreSQL 9.6}}
            \item Dirección IP: 172.40.0.20
            \item Puerto de escucha: 5432
            \item Nombre de la base de datos: facturacion
            \item Usuario: admin
            \item Contraseña: admin
        \end{itemize}
    \item Datawarehouse:
        \begin{itemize}
            \item Imagen: \underline{\href{https://hub.docker.com/_/postgres/}{PostgreSQL 9.6}}
            \item Dirección IP: 172.40.0.30
            \item Puerto de escucha: 5432
            \item Nombre de la base de datos: datawarehouse
            \item Usuario: admin
            \item Contraseña: admin
        \end{itemize}
    \item Pentaho \emph{BI Server} :
        \begin{itemize}
            \item Imagen: \underline{\href{https://hub.docker.com/r/wmarinho/pentaho/}{Pentaho BI Server}}
            \item Dirección IP: 172.40.0.40
            \item Puerto de escucha: 7070
        \end{itemize}
\end{itemize}

\section{Bases de datos operacionales}

Los sistemas de facturación de \dq{PatSur} presentan distintos modelos de datos, por lo cual la definición de las tablas de las bases de datos y su correspondiente carga de datos serán procesos distintos.

Todos los \emph{scipts} a los que se haga referencia se encontrarán en el directorio \textbf{\dq{scripts}}. Los \emph{scripts} correspondientes a la carga de datos tanto de los sistemas operacionales como del datawarehouse se encontrarán en el subdirectorio \textbf{\dq{scripts/cargas}}   

\subsection{Sistema viejo}

\subsubsection{Definición}

El primer sistema con el que dispuso la empresa tenía la siguiente estructura de tablas:

\begin{itemize}
    \item Cliente(\textbf{nro\_cliente}, nombre, tipo, direccion)
    \item Categoria(\textbf{nro\_categ}, descripcion)
    \item Venta(\textbf{nro\_factura}, fecha\_venta, \underline{nro\_cliente}, nombre, forma\_pago)
    \item Detalle\_Venta(\underline{\textbf{nro\_factura}}, \underline{\textbf{nro\_producto}}, descripcion, unidad, precio)
    \item Producto(\textbf{nro\_producto}, nombre, \underline{nro\_categ}, precio\_actual)
\end{itemize}

El \emph{script} correspondiente para la definición de las tablas previamente definidas tiene el nombre \textbf{tablas\_sistema\_viejo.sql} 

\subsubsection{Carga de datos}

\emph{Los siguientes \emph{scripts} mencionados se encuentran en el subdirectorio \dq{scripts/cargas/sistema\_viejo} y en \dq{scripts/cargas/sistema\_viejo/sql}} 

~\\

A través de \emph{scripts} escritos en Python (\textbf{productos.py} y \textbf{categorias.py}), se pudieron extraer datos de la \emph{API} de MercadoLibre \autocite{api} para disponer de datos con cierto grado de veracidad. Estos \emph{scripts} se encargan de consultar la fuente de datos, y de armar \emph{scripts} en SQL (\textbf{sql/productos} y \textbf{sql/categorias.sql}) para la inserción en la tabla correspondiente (adecuando los datos acordemente al modelo de la base de datos)  

MercadoLibre maneja un árbol de \textbf{categorías} muy amplio y con muchas ramas, de las cuales solamente se exploraron y utilizaron unas pocas para este trabajo. También se obtuvieron \textbf{productos} de dichas categorías

\pagebreak

\vspace*{5mm}
\begin{lstlisting}[title=Cantidad de categorías y productos disponibles en el sistema de facturación viejo]
facturacion=# SELECT COUNT(*) FROM categoria;
 count 
-------
    30
(1 row)

facturacion=# SELECT COUNT(*) FROM producto;
 count 
-------
  1450
(1 row)
\end{lstlisting}

Con respecto a los \textbf{clientes}, se utilizó un sitio generador de datos \autocite{data} para crear registros de clientes ficticios (\textbf{sql/clientes.sql}). 

Por último, los archivos \textbf{sql/venta.sql} y \textbf{sql/detalle\_venta.sql} contienen las definiciones de las funciones \textbf{insertar\_venta(int)} e \textbf{insertar\_detalle\_venta(int)}. Ambas funciones reciben un valor entero como parámetro, que indica la cantidad de registros que se quieren crear (tomando valores aleatorios de la base). Estas funciones son acumulativas, entonces se pueden ejecutar cuantas veces se desee para poblar las tablas de ventas y detalles de ventas.

\vspace*{5mm}
\begin{lstlisting}[title=Inserción de 50 ventas aleatorias en la BD]
facturacion=# SELECT insertar_ventas(50);
NOTICE:  [1/50] INSERTANDO VENTA (2007-10-03, 862512, 4118, Linda Acosta, EFECTIVO)
NOTICE:  [2/50] INSERTANDO VENTA (2002-07-29, 143370, 3458, Jeremy Pacheco, CREDITO)
NOTICE:  [3/50] INSERTANDO VENTA (2006-02-15, 993978, 2414, Lisandra Bernard, DEBITO)
...
\end{lstlisting}

\vspace*{5mm}
\begin{lstlisting}[title=Inserción de 300 detalles de ventas aleatorias en la BD]
facturacion=# SELECT insertar_detalle_venta(300);
NOTICE:  [1/300] INSERTANDO DETALLE (333538, 639930615, 19, 169.99)
NOTICE:  [2/300] INSERTANDO DETALLE (743695, 644739590, 41, 72)
NOTICE:  [3/300] INSERTANDO DETALLE (124334, 712205536, 54, 419500)
...
\end{lstlisting}

\subsection{Sistema actual}

\subsubsection{Definición}

El sistema desarrollado en el 2010 por la consultora cuenta con el siguiente esquema:

\begin{itemize}
    \item Cliente(\textbf{cod\_cliente}, nombre, \underline{cod\_tipo}, direccion)
    \item Tipo\_Cliente(\textbf{cod\_tipo}, descripcion)
    \item Producto(\textbf{cod\_producto}, nombre, \underline{cod\_categoria}, \underline{cod\_subcategoria}, precio\_actual)
    \item Categoria(\textbf{cod\_categoria}, \textbf{cod\_subcategoria}, descripcion)
    \item Venta(\textbf{id\_factura}, fecha\_venta, \underline{cod\_cliente}, nombre, \underline{cod\_medio\_pago})
    \item Detalle\_Venta(\underline{\textbf{id\_factura}}, \underline{\textbf{cod\_producto}}, descripcion, unidad, precio)
    \item Medio\_Pago(\textbf{cod\_medio\_pago}, descripcion, valor, unidad, tipo\_operacion)
\end{itemize}

El \emph{script} correspondiente para la definición de las tablas previamente definidas tiene el nombre \textbf{tablas\_sistema\_actual.sql} 

\subsubsection{Carga de datos}

Los \emph{scripts} realizados para la carga de datos de este sistema son idénticos a los del sistema anterior, adaptándolos a los distintas representaciones de los datos. Para los \textbf{productos} y \textbf{categorías}, también se extrajeron datos de MercadoLibre, agregando como valor de \emph{subcategoría} un valor secuencial entre 1 y 5 por cada categoría (es decir, cada categoria tiene 5 subcategorías secuenciales)

\vspace*{5mm}
\begin{lstlisting}[title=Cantidad de categorías y productos disponibles en el sistema de facturación actual]
facturacion=# SELECT COUNT(*) FROM categoria;
 count 
-------
   150
(1 row)

facturacion=# SELECT COUNT(*) FROM producto;
 count 
-------
  1454
(1 row)
\end{lstlisting}

La carga de \textbf{clientes} es igual (\textbf{sql/clientes.sql}), teniendo en cuenta cargar clientes repetidos para después figuren en las \emph{tablas de equivalencia} del datawarehouse. 

Varios \textbf{medios de pago} fueron registrados mediante el \emph{script} \textbf{sql/medio\_pago.sql}, al igual que \textbf{tipos de cliente} (\textbf{sql/tipo\_cliente.sql})   

Las funciones para creaciones de \textbf{ventas} y \textbf{detalles de ventas} mantienen la misma firma que las implementadas en el sistema viejo (\textbf{sql/venta.sql} y \textbf{sql/detalle\_venta.sql}, respectivamente). 

\vspace*{5mm}
\begin{lstlisting}[title=Inserción de 15 ventas aleatorias en la BD]
facturacion=# SELECT insertar_ventas(15);
NOTICE:  [1/15] INSERTANDO VENTA (2015-07-25, 119357, 252269, Ifeoma G. Collins, Débito)
NOTICE:  [2/15] INSERTANDO VENTA (2014-07-31, 585847, 967234, Keane K. Hodge, Bitcoin)
NOTICE:  [3/15] INSERTANDO VENTA (2013-05-01, 501180, 639678, Edward T. Carey, Crédito)
...
\end{lstlisting}

\vspace*{5mm}
\begin{lstlisting}[title=Inserción de 100 detalles de ventas aleatorias en la BD]
facturacion=# SELECT insertar_detalle_venta(100);
NOTICE:  [1/100] INSERTANDO DETALLE (462363, 678175295, 10, 2249)
NOTICE:  [2/100] INSERTANDO DETALLE (660157, 655324667, 77, 340)
NOTICE:  [3/100] INSERTANDO DETALLE (164479, 612842726, 38, 8499)
...
\end{lstlisting}



\section{Datawarehouse}

\subsection{Marco teórico}

Un Datawarehouse o almacén de datos es definido por \autocite{elmasri-dw} como \emph{una colección de datos orientada al sujeto, integrada, no volátil y de tiempo variable para el soporte de las decisiones de los directivos} 

Existen dos esquemas posibles para la implementación de un datawarehouse. El primero, el \textbf{esquema estrella}, consiste en tener una única \emph{Tabla de Hechos} como objeto central del almacén, y distintas \emph{Tablas de Dimensión} circundantes. La Tabla de Hechos se relaciona con las dimensiones mediante una \emph{clave foránea}, pero las Tablas de Dimesión no se relacionan entre sí.

En este esquema se \dq{desnormalizan} los datos de las bases de datos operacionales de las cuales se nutren el datawarehouse para tener mayor performance (pero con redundancia en los datos)

El otro esquema posible es el \textbf{copo de nieve}. Esta alternativa propone que las Tablas de Dimensión tengan Tablas de Dimesión asociadas a ella. De esta forma, los datos se mantienen normalizados y las relaciones más pequeñas.

Este último esquema es el adecuado para implementar el datawarhouse que requiere \dq{PatSur}.

\subsection{Definición}

Se utilizó la siguiente estructura de tablas.

\begin{itemize}
    \item Categoria(\textbf{id\_categoria}, \textbf{id\_subcategoria}, descripcion)
    \item Producto(\textbf{id\_producto}, \underline{id\_categoria}, \underline{id\_subcategoria}, descripcion)
    \item Medios(\textbf{id\_medio\_pago}, descripcion)
    \item Region(\textbf{id\_region}, descripcion)
    \item Provincia(\textbf{id\_provincia}, \underline{id\_region}, descripcion)
    \item Ciudad(\textbf{id\_ciudad}, \underline{id\_provincia}, descripcion)
    \item DistribucionGeografica(\textbf{id\_sucursal}, \underline{id\_ciudad}, descripcion)
    \item TipoCliente(\textbf{id\_tipo}, descripcion)
    \item Cliente(\textbf{id\_cliente}, \underline{id\_tipo}, nombre)
    \item Tiempo(\textbf{id\_tiempo}, anio, mes)
    \item Venta(fecha, \underline{\textbf{id\_tiempo}}, \underline{\textbf{id\_factura}}, \underline{\textbf{id\_cliente}}, \underline{\textbf{id\_producto}}, \underline{\textbf{id\_sucursal}}, 
        
        \underline{\textbf{id\_medio\_pago}}, monto\_vendido, cantidad\_vendida, descripcion)
\end{itemize}

\subsection{Carga de datos}

\emph{Todos los \emph{scripts} mencionados a continuación se pueden encontrar en el subdirectorio \dq{scripts/cargas/datawarehouse}}

~\\

Para poder relizar la integración de los datos se decidió utilizar tablas de equivalencia para las tablas Producto y Cliente de ambos sistemas de facturación.

Se utilizó la siguiente estructura:

\begin{itemize}
    \item TECliente(cliente\_s\_v, cliente\_s\_n, cliente\_d\_w)
    \item TEProducto(producto\_s\_v, producto\_s\_n, producto\_d\_w)
\end{itemize}

Para las tablas de categoría, región, provincia, ciudad, sucursal, medio de pago y tipo de cliente se utilizaron los mismos datos en ambos sistemas o se generaron datos para ingresar en el datawarehouse.

Luego se implementaron \emph{scripts} para que, desde el datawarehouse, se realicen consultas remotas a los sistemas actual y viejo; se realizará la equivalencia y se insertarán en las tablas de equivalencia, detallado a continuacion. 

\pagebreak

\begin{lstlisting}[title=Inserción a la tabla de equivalencia de cliente]

-- Conexión con el sistema de facturación viejo
SELECT DBLINK_CONNECT(
	'sistema_viejo',
	'host=172.40.0.10 port=5432 password=admin user=admin dbname=facturacion'
);

-- Conexión con el sistema actual
SELECT DBLINK_CONNECT(
	'sistema_actual',
	'host=172.40.0.20 port=5432 password=admin user=admin dbname=facturacion'
);

-- Inserta en la tabla de equivalencia de clientes
-- teniendo en cuenta que pueden haber clientes con el mismo nombre,
-- en cuyo caso los inserta en la misma fila

INSERT INTO te_cliente (cliente_s_n, cliente_s_v)
SELECT
    consulta_actual.cod_cliente,
    consulta_viejo.nro_cliente
FROM
    DBLINK('sistema_actual', 'SELECT cod_cliente, nombre FROM cliente')
    AS consulta_actual(cod_cliente integer, nombre varchar(50))
    FULL OUTER JOIN
        DBLINK('sistema_viejo', 'select nro_cliente, nombre from cliente')
        AS consulta_viejo(nro_cliente integer, nombre varchar(50))
    ON consulta_viejo.nombre = consulta_actual.nombre;
    
\end{lstlisting}

Para la inserción en la tabla de equivalencia de \textbf{productos} el procedimiento es similar, considerando el nombre de los productos.

Los scripts se pueden encontrar en \textbf{te\_cliente.sql} y \textbf{te\_producto.sql}

Se corren ambos \emph{scripts} y ya se dispone de las tablas de equivalencia para relizar el \textbf{ETL}.

Con respecto al resto de las tablas, se utilizan los scripts homónimos a las tablas a poblar. Por ejemplo: \textbf{categoria.sql}, \textbf{region.sql}


\subsubsection{\emph{ETL}}

Para el \emph{ETL} (\emph{Extraction, Transformation, Load}, \emph{Extracción, Transformación, Carga}) se crea una tabla temporal para albergar las ventas de un año y mes determinados. Acto seguido se realizan las inserciones a las tablas de Cliente, Producto, Venta, y Tiempo. Este procedimiento es realizado a través de una función a la cual se le pasa el mes y año a insertar en el datawarehouse.

La creación de la tabla temporal, dentro de la función se realiza de la siguiente manera:

\pagebreak

\begin{lstlisting}[title=Creación de la Tabla Temporal]
-- Conexión con el sitio del cual conseguir los datos
PERFORM DBLINK_CONNECT(
    'sistema_actual',
    'host=172.40.0.20 port=5432 password=admin user=admin dbname=facturacion'
);

-- Creación tabla tempora
CREATE TABLE tmp_ventas AS
    SELECT
    * 
    FROM 
    DBLINK(
        'sistema_actual', 
        'SELECT
        fecha_venta,
        v.id_factura,
        v.cod_cliente,
        dv.cod_producto,
        cod_medio_pago,
        unidad * precio AS monto_vendido,
        unidad AS cantidad_vendida,
        p.nombre AS nombre_producto,
        p.cod_categoria,
        p.cod_subcategoria,
        precio,
        c.nombre AS nombre_cliente,
        c.cod_tipo
        FROM venta v, cliente c, detalle_venta dv, producto p
        WHERE 
        v.id_factura = dv.id_factura
        AND v.cod_cliente = c.cod_cliente
        AND dv.cod_producto = p.cod_producto 
        AND DATE_PART(''month'', fecha_venta) =  '|| $2 ||'
                AND DATE_PART(''year'', fecha_venta) = '|| $1 ||'
        ORDER BY 1'
    ) AS st(
        fecha_venta date,
        id_factura integer,
        cod_cliente integer,
        cod_producto integer,
        cod_medio_pago integer,
        monto_vendido real,
        cantidad_vendida integer,
        nombre_producto varchar(100),
        cod_categoria integer,
        cod_subcategoria integer,
        precio real,
        nombre_cliente varchar(100),
        cod_tipo integer
    );
\end{lstlisting}

La selección de datos y el armado de la tabla temporal considera la estructura del sitio del cual se extraen los datos, lo cual hace que los campos que selecciona no sean los mismos cuando se extrae del sistema viejo que cuando se extrae del sistema actual. 

Una vez cargada la tabla temporal, se realizan las inserciones a Producto, Cliente y Venta, para luego eliminarla (\emph{DROP TABLE}).

\emph{Ambas funciones ETL están en los \emph{scripts} \textbf{poblar\_dw\_sistema\_viejo.sql} \newline y \textbf{poblar\_dw\_sistema\_nuevo.sql}.}

Para facilitar la implementación de la herramienta de BI, se generaron vistas que reduzcan el esquema de Copo de Nieve planteado en el datawarehouse a uno de Estrella. Estas definciones se encuentran en el archivo \textbf{vistas\_datawarehouse.sql}

\subsection{Explotación}

Teniendo el datawarhouse creado y con datos cargados a partir de las bases de datos operacionales (luego de convertirlos con los procesos de ETL definidos), se pueden realizar distintas consultas que provean soporte para la toma de decisiones de \dq{PatSur}

\subsection{Distintas perspectivas de las ventas}

Una diferencia vital entre un datawarehouse y una base de datos operativa es la capacidad del primero de soportar operaciones \textbf{OLAP}. El \emph{área de procesamiento analítico en línea} (\emph{Online Analytical Processing}, \emph{OLAP}) trata de las herramientas y de las técnicas para el análisis de los datos que pueden dar respuestas casi instantáneas a las consultas que soliciten datos resumidos, aunque la base de datos (datawarehouse) sea extremedamente grande \autocite{silberschatz-olap}

Para mostrar distintas perspectivas y agrupaciones posibles con respecto a las ventas de la empresa, se puede utilizar una de las extensiones de SQL-1999: la claúsula \textbf{CUBE} 

Debido a la extensión de la consulta, se la puede encontrar en el archivo \textbf{consultas/olap.sql} 

Los siguientes criterios por los cuales se agrupa la cantidad de unidades vendidas y el monto total vendido son:

\begin{itemize}
    \item Fecha
    \item Cliente
    \item Producto
    \item Medio de pago
    \item Sucursal
    \item Ciudad
    \item Provincia
\end{itemize}

Se toma un criterio por vez, y también se agrupa por las distintas combinaciones posibles



\subsection{Clientes que generan mayores ingresos}

La consulta del archivo \textbf{consultas/clientes\_mas\_compradores.sql} utiliza otra extensión de SQL-1999, \textbf{RANK()} para indicar numéricamente un orden de los registros según un criterio indicado  

\vspace*{5mm}
\begin{lstlisting}[title=Clientes que generan mayores ingresos para la empresa]
SELECT 
    cli.id_cliente AS "ID Cliente", 
    cli.nombre AS "Nombre", 
    CAST(SUM(ven.monto_vendido) AS bigint) AS "Total vendido", 
    RANK() OVER (ORDER BY SUM(ven.monto_vendido) DESC) AS "Mayores compradores" 
FROM venta AS ven 
    JOIN cliente AS cli ON ven.id_cliente=cli.id_cliente 
GROUP BY cli.id_cliente;
\end{lstlisting}




\section{Herramienta de \emph{BI}: \emph{Pentaho}}

\emph{Pentaho BI Suite} es un conjunto de programas para el soporte de la toma de decisiones y \emph{Business Intelligence} \autocite{wikipedia-pentaho} de la empresa Hitachi. Todas las herramientas de la suite corren sobre la \emph{JVM} (por lo tanto es necesario contar con Java instalado).

Para este trabajo de laboratorio se utilizaron las herramientas \underline{\href{https://mondrian.pentaho.com/documentation/workbench.php}{\emph{Schema Workbench}}} y \underline{\href{https://help.pentaho.com/Documentation/7.1/Installation}{\emph{BI Server}}} (este último se implementó mediante contenedores de \docker{}, con las configuraciones descriptas en la sección 1)

La herramienta \emph{Schema Worbench} se utilizó para definir el esquema que contiene el cubo OLAP, cuyas medidas y dimensiones son las requeridas por \dq{PatSur}. Estas son:

\begin{itemize}
    \item Medidas:
        \begin{enumerate}
            \item Unidades vendidas
            \item Monto recaudado
        \end{enumerate}
    \item Dimensiones:
        \begin{enumerate}
            \item Fecha
            \item Sucursales
            \item Clientes
            \item Productos
            \item Medios de pago
        \end{enumerate}
\end{itemize}


\subsection*{Integración de \emph{Schema Workbench} con \emph{PostgreSQL}}

Cabe aclarar que para conectarse a una base de datos \emph{PostgreSQL} desde el \emph{Schema Workbench} hace falta descargar los \underline{\href{https://jdbc.postgresql.org/}{drivers JDBC para el motor}} y ubicarlos en el directorio \dq{drivers} dentro del directorio de instalación de \emph{Schema Workbench} 

\subsection{Definición del esquema y del cubo OLAP}

Una vez instalado el driver correspondiente para el motor y con el datawarehouse corriendo, seguir la siguiente secuencia de pasos para poder conectarse a la base de datos y crear el cubo OLAP:

\begin{enumerate}
    \item Crear la conexión con el datawarehouse
    \begin{enumerate}
        \item Hacer click en \texttt{Options}
        \item Seleccionar \texttt{Connection...}
        \item Los datos para la conexión con el datawarehouse son los siguientes:
        \begin{itemize}
            \item Connection Type: PostgreSQL
            \item \textbf{Connection Name}: conn\_datawarehouse 
            \item Host Name: 172.40.0.30
            \item Database Name: datawarehouse
            \item Port Number: 5432
            \item User Name: admin
            \item Password: admin
        \end{itemize}
        \item Hacer click en \texttt{OK}
    \end{enumerate}
     \item Crear un nuevo esquema
    \begin{enumerate}
        \item Hacer click en \texttt{File}
        \item Seleccionar \texttt{New}
        \item Seleccionar \texttt{Schema}
    \end{enumerate}
    \item Crear un nuevo cubo
    \begin{enumerate}
        \item Click derecho sobre el esquema recién creado
        \item Seleccionar \texttt{Add cube}
    \end{enumerate}
    \item Definir la Tabla de Hechos para el cubo
    \begin{enumerate}
        \item Click derecho sobre el cubo
        \item Seleccionar \texttt{Add Table}
    \end{enumerate}
    \item Definir las medidas del cubo
    \begin{enumerate}
        \item Click derecho sobre el cubo
        \item Seleccionar \texttt{Add Measure}
        \item Definir qué columna de la Tabla de Hechos será utilizada como medida y con qué operación (cantidad de valores distintos, suma, cuenta, etc.)
    \end{enumerate}
    \item Definir las dimensiones del cubo
    \begin{enumerate}
        \item Click derecho sobre el cubo
        \item Seleccionar \texttt{Add Dimension}
        \item Cada dimensión tiene una \emph{jerarquía}, la cual debe estar relacionada con una tabla del datawarehouse 
        \item Cada jerarquía también debe tener por lo menos un nivel definido; cada nivel debe estar relacionado con una columna de la tabla de la jerarquía
    \end{enumerate}
    \item Publicar el esquema en el servidor
    \begin{enumerate}
        \item Hacer click en \texttt{File}
        \item Seleccionar \texttt{Publish...}
        \item Los datos que se deben ingresar para publicar el esquema son los siguientes:
        \begin{itemize}
            \item URL: localhost:7070
            \item Credentials:
                \begin{itemize}
                    \item User: admin
                    \item Password: password
                \end{itemize}
        \end{itemize}
        \item Hacer click en \texttt{OK}
    \end{enumerate}
\end{enumerate}

\begin{displayquote}
    \textbf{ACLARACIÓN}: cada vez que se publique una nueva versión del cubo al servidor, se debe refrescar la \emph{caché Mondrian} del servidor, para ello, desde la pantalla de inicio del \emph{BI Server}, hacer click en \texttt{Tools}, luego en \texttt{Refresh}, y seleccionar \texttt{Mondrian Schema Cache}     
\end{displayquote}

\subsubsection{Exportando el cubo de \dq{PatSur}}

En el directorio \dq{scripts/pentaho} se encuentra un archivo llamado 

\textbf{patsur\_olap.mondrian.xml} que corresponde al esquema que contiene el cubo confeccionado para \dq{PatSur}.

Para poder publicarlo en el servidor, abrir el archivo desde \emph{Schema Workbench} y realizar los pasos para publicarlo en el servidor. 


\subsection{Visualización del cubo}

Una vez publicado el esquema en el servidor, navegar a la dirección \emph{localhost:7070} e ingresar las credenciales previamente descriptas (\emph{Username}: admin, \emph{Password}: password). 

Antes de visualizar el cubo, se debe crear la conexión con el datawarehouse. Para ello, seguir los siguientes pasos:

\begin{enumerate}
    \item Hacer click en \texttt{Manage Data Sources}
    \item Hacer click en el engranaje
    \item Seleccionar \texttt{New Connection...}
    \item Ingresar \textbf{los mismos datos} con los cuales se definió la conexión al datawarehouse en el \emph{Schema Workbench}   
    \item Volver a la pantalla de inicio
    \item Hacer click en \texttt{Create New}
    \item Seleccionar \texttt{JPivot View} 
    \item Seleccionar el esquema y el cubo adecuado, y hacer click en \texttt{OK} 
\end{enumerate}

En la pestaña de \texttt{JPivot View} se puede modificar la visualización del cubo para agregar o eliminar medidas o dimensiones, filtrar, ordenar, y luego guardar esas vistas creadas 

Con esta herramienta es posible generar vistas que provean la misma información que las consultas de la sección anterior.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FIN DOCUMENTO, AHORA REFERENCIAS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\printbibliography

\end{document}

